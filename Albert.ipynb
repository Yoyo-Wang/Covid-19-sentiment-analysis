{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from transformers import BertTokenizer\n",
    "from transformers import AlbertTokenizer\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import transformers\n",
    "# from transformers import BertForSequenceClassification\n",
    "from transformers import AlbertForSequenceClassification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./Corona_NLP_train.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_tags(s):\n",
    "    hashes = re.findall(r\"#(\\w+)\", s)\n",
    "    return \" \".join(hashes)\n",
    "df['hashtags'] = df['OriginalTweet'].apply(lambda x : extract_hash_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions(s):\n",
    "    hashes = re.findall(r\"@(\\w+)\", s)\n",
    "    return \" \".join(hashes)\n",
    "df['mentions'] = df['OriginalTweet'].apply(lambda x : extract_mentions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = df['Sentiment']\n",
    "\n",
    "\n",
    "\n",
    "encoding = {'Extremely Negative': 0,\n",
    "            'Negative': 0,\n",
    "            'Neutral': 1,\n",
    "            'Positive':2,\n",
    "            'Extremely Positive': 2\n",
    "           }\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "\n",
    "y.replace(encoding, inplace=True)\n",
    "\n",
    "df['encoded_sentiment'] = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].apply(lambda x: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xval, ytrain, yval = train_test_split(df['OriginalTweet'], df['encoded_sentiment'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AlbertTokenizer.from_pretrained(\"albert-base-v2\",do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/anaconda/envs/py37_default/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    xtrain, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=50, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    xval, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=50, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(ytrain.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(yval.values)\n",
    "\n",
    "\n",
    "# Pytorch TensorDataset Instance\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=128)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b361912eee45698ea3fd6e3522be71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training Acc: 0.815459377372817\n",
      "Training Acc: 0.8044217687074829\n",
      "Training loss: 0.7392052764347358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training Acc: 0.8783295368261199\n",
      "Training Acc: 0.8566569484936832\n",
      "Training loss: 0.44691880382308663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training Acc: 0.9074259681093394\n",
      "Training Acc: 0.8771865889212828\n",
      "Training loss: 0.34922012761812804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training Acc: 0.9239483675018982\n",
      "Training Acc: 0.8872691933916423\n",
      "Training loss: 0.2876739645766657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training Acc: 0.9373728170083523\n",
      "Training Acc: 0.8929786200194364\n",
      "Training loss: 0.23785059052959892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 6', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n",
      "Training Acc: 0.9452695520121488\n",
      "Training Acc: 0.8888483965014577\n",
      "Training loss: 0.19765566465582035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 7', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n",
      "Training Acc: 0.9604555808656037\n",
      "Training Acc: 0.8923712342079689\n",
      "Training loss: 0.16141088664993758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 8', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n",
      "Training Acc: 0.9682915717539863\n",
      "Training Acc: 0.891885325558795\n",
      "Training loss: 0.13317895453971948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 9', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n",
      "Training Acc: 0.9753378891419894\n",
      "Training Acc: 0.891885325558795\n",
      "Training loss: 0.1107561585977096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 10', max=258.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10\n",
      "Training Acc: 0.9782536066818527\n",
      "Training Acc: 0.891642371234208\n",
      "Training loss: 0.09372863327983276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0].to(device),\n",
    "                  'attention_mask': batch[1].to(device),\n",
    "                  'labels':         batch[2].to(device),\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "    \n",
    "    # write training loss\n",
    "    train_loss, train_predictions, train_vals = evaluate(dataloader_train)\n",
    "#     writer.add_scalar(\"logs/AlbertLoss/train\", loss_train_avg, epoch)\n",
    "    x = 0\n",
    "    preds_flat = np.argmax(train_predictions, axis=1).flatten()\n",
    "    labels_flat = train_vals.flatten()\n",
    "    for i in range(len(preds_flat)):\n",
    "        if labels_flat[i] == preds_flat[i]:\n",
    "            x += 1\n",
    "    acc_train_avg = x/len(preds_flat)\n",
    "    writer.add_scalar(\"logs/AlbertAcc/train\", acc_train_avg, epoch)\n",
    "    tqdm.write(f'Training Acc: {acc_train_avg}')\n",
    "    # calculate validatin loss and write\n",
    "    val_loss, val_predictions, val_vals = evaluate(dataloader_validation)\n",
    "#     writer.add_scalar(\"logs/AlbertLoss/val\", val_loss, epoch)\n",
    "    x = 0\n",
    "    preds_flat = np.argmax(val_predictions, axis=1).flatten()\n",
    "    labels_flat = val_vals.flatten()\n",
    "    for i in range(len(preds_flat)):\n",
    "        if labels_flat[i] == preds_flat[i]:\n",
    "            x += 1\n",
    "    acc_val_avg = x/len(preds_flat)\n",
    "    writer.add_scalar(\"logs/AlbertAcc/val\", acc_val_avg, epoch)\n",
    "    tqdm.write(f'Training Acc: {acc_val_avg}')\n",
    "    \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x7ffa890bc690>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss =  0.36984656063409954\n",
      "Val F1 =  0.8783166531625034\n"
     ]
    }
   ],
   "source": [
    "val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "val_f1 = f1_score_func(predictions, true_vals)\n",
    "print('Val Loss = ', val_loss)\n",
    "print('Val F1 = ', val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_classes = encoder.classes_\n",
    "predicted_category = [encoded_classes[np.argmax(x)] for x in predictions]\n",
    "true_category = [encoded_classes[x] for x in true_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score =  0.8787657920310982\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for i in range(len(true_category)):\n",
    "    if true_category[i] == predicted_category[i]:\n",
    "        x += 1\n",
    "        \n",
    "print('Accuracy Score = ', x / len(true_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mat = confusion_matrix(y_true = true_category, y_pred = predicted_category, labels=list(encoded_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff298971d90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.DataFrame(confusion_mat, index = list(encoded_classes),columns = list(encoded_classes))\n",
    "sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_default/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('./Corona_NLP_test.csv', encoding='latin-1')\n",
    "\n",
    "test_df['hashtags'] = test_df['OriginalTweet'].apply(lambda x : extract_hash_tags(x))\n",
    "\n",
    "test_df['mentions'] = test_df['OriginalTweet'].apply(lambda x : extract_mentions(x))\n",
    "\n",
    "ytest = test_df['Sentiment']\n",
    "\n",
    "ytest.replace(encoding, inplace=True)\n",
    "\n",
    "test_df['encoded_sentiment'] = encoder.fit(ytest)\n",
    "\n",
    "test_df['OriginalTweet'] = test_df['OriginalTweet'].apply(lambda x: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))\n",
    "\n",
    "xtest = test_df.OriginalTweet\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    xtest, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=50, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(ytest.values)\n",
    "\n",
    "\n",
    "# Pytorch TensorDataset Instance\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_testing = DataLoader(dataset_test, \n",
    "                                   sampler=SequentialSampler(dataset_test), \n",
    "                                   batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss =  0.39348272879918417\n",
      "Test F1 =  0.8638112889540698\n"
     ]
    }
   ],
   "source": [
    "test_loss, predictions, true_vals = evaluate(dataloader_testing)\n",
    "test_f1 = f1_score_func(predictions, true_vals)\n",
    "print('Test Loss = ', test_loss)\n",
    "print('Test F1 = ', test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score =  0.8644023170089521\n"
     ]
    }
   ],
   "source": [
    "encoded_classes = encoder.classes_\n",
    "predicted_category = [encoded_classes[np.argmax(x)] for x in predictions]\n",
    "true_category = [encoded_classes[x] for x in true_vals]\n",
    "x = 0\n",
    "for i in range(len(true_category)):\n",
    "    if true_category[i] == predicted_category[i]:\n",
    "        x += 1\n",
    "        \n",
    "print('Accuracy Score = ', x / len(true_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.5422733, -1.5299261, -1.3479844],\n",
       "       [-1.5763834, -1.0507784,  3.5031831],\n",
       "       [-1.5015876, -1.6305078,  3.9209301],\n",
       "       ...,\n",
       "       [-2.1618435,  2.6784325, -1.2939514],\n",
       "       [ 2.6491485, -1.5153286, -1.779066 ],\n",
       "       [-1.62269  , -1.5236105,  4.012443 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./predictions/albert.csv\", predictions, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
