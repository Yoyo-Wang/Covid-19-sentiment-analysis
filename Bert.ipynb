{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./Corona_NLP_train.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_tags(s):\n",
    "    hashes = re.findall(r\"#(\\w+)\", s)\n",
    "    return \" \".join(hashes)\n",
    "df['hashtags'] = df['OriginalTweet'].apply(lambda x : extract_hash_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions(s):\n",
    "    hashes = re.findall(r\"@(\\w+)\", s)\n",
    "    return \" \".join(hashes)\n",
    "df['mentions'] = df['OriginalTweet'].apply(lambda x : extract_mentions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = df['Sentiment']\n",
    "\n",
    "\n",
    "\n",
    "encoding = {'Extremely Negative': 0,\n",
    "            'Negative': 0,\n",
    "            'Neutral': 1,\n",
    "            'Positive':2,\n",
    "            'Extremely Positive': 2\n",
    "           }\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "\n",
    "y.replace(encoding, inplace=True)\n",
    "\n",
    "df['encoded_sentiment'] = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...\n",
       "1    advice Talk to your neighbours family to excha...\n",
       "2    Coronavirus Australia: Woolworths to give elde...\n",
       "3    My food stock is not the only one which is emp...\n",
       "4    Me, ready to go at supermarket during the #COV...\n",
       "Name: OriginalTweet, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.OriginalTweet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].apply(lambda x: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        Gahan and and\n",
       "1    advice Talk to your neighbours family to excha...\n",
       "2    Coronavirus Australia Woolworths to give elder...\n",
       "3    My food stock is not the only one which is emp...\n",
       "4    Me ready to go at supermarket during the COVID...\n",
       "Name: OriginalTweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.OriginalTweet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xval, ytrain, yval = train_test_split(df['OriginalTweet'], df['encoded_sentiment'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/anaconda/envs/py37_default/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    xtrain, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=50, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    xval, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=50, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(ytrain.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(yval.values)\n",
    "\n",
    "\n",
    "# Pytorch TensorDataset Instance\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=3,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=128)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 1\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23057d3485834cb7aa25f8b66aff1a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=258.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch 1\n",
      "Training loss: 0.3204379112336987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0].to(device),\n",
    "                  'attention_mask': batch[1].to(device),\n",
    "                  'labels':         batch[2].to(device),\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss =  0.3825511776483976\n",
      "Val F1 =  0.870216445180939\n"
     ]
    }
   ],
   "source": [
    "val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "val_f1 = f1_score_func(predictions, true_vals)\n",
    "print('Val Loss = ', val_loss)\n",
    "print('Val F1 = ', val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_classes = encoder.classes_\n",
    "predicted_category = [encoded_classes[np.argmax(x)] for x in predictions]\n",
    "true_category = [encoded_classes[x] for x in true_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score =  0.8711127308066083\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "for i in range(len(true_category)):\n",
    "    if true_category[i] == predicted_category[i]:\n",
    "        x += 1\n",
    "        \n",
    "print('Accuracy Score = ', x / len(true_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mat = confusion_matrix(y_true = true_category, y_pred = predicted_category, labels=list(encoded_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbc849631d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAD4CAYAAAAw/yevAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASE0lEQVR4nO3dYYxd9Xnn8e8PllIagkrEws7a3oW27rZAVUdECClSlJbdxRtVa/oiktMqoAp1KkTaIPVFoS+2zQtXvNimWlYCyVUijJQNsjaJsKLQLmslirolgBOxAeNSrMCGqR2sNq3iSBXNzDz74h7ErTNz5874Tu7ff38/0l9z73POuec/I/T44Tn/c0+qCklSWy6Z9wQkST/M5CxJDTI5S1KDTM6S1CCTsyQ16F9s9wn+8ehBl4Nss3f/p0/Mewrdu+Ynrpr3FC4K3/mHEznfz/jB335r6pxz2TU/dd7n2y5WzpLUoG2vnCXpR2p1Zd4zmAmTs6S+rCzPewYzYXKW1JWq1XlPYSZMzpL6smpylqT2WDlLUoO8IChJDbJylqT2lKs1JKlBnVwQ9A5BSX2p1enHBEl+PMlzSf5vkuNJPjHE35Pk6SSvDj+vHjvmwSQnk7yS5I6x+C1JXhy2PZxkw9vGTc6S+rK6Mv2Y7C3gl6vqF4E9wN4ktwEPAEerajdwdHhPkhuB/cBNwF7gkSSXDp/1KLAI7B7G3o1ObnKW1JcZVc418v3h7WXDKGAfcGiIHwLuHF7vA56oqreq6jXgJHBrkgXgqqp6pkbPBXx87Jh1mZwl9WVleeqRZDHJsbGxOP5RSS5N8gJwBni6qp4Frquq0wDDz2uH3XcAb4wdvjTEdgyvz41P5AVBSX3ZxAXBqjoIHJywfQXYk+QngS8kuXnCx63VR64J8YlMzpK6Msqns/7M+ockX2HUK34zyUJVnR5aFmeG3ZaAXWOH7QRODfGda8Qnsq0hqS+zW63xL4eKmSRXAP8e+CvgCHD3sNvdwJPD6yPA/iSXJ7mB0YW/54bWx9kktw2rNO4aO2ZdVs6S+jK7dc4LwKFhxcUlwOGq+mKSZ4DDSe4Bvg18GKCqjic5DLwMLAP31Ttl/L3AY8AVwFPDmMjkLKkvM7p9u6q+Cbx3jfjfAbevc8wB4MAa8WPApH71DzE5S+rLyg/mPYOZMDlL6ksnt2+bnCX1xW+lk6QGWTlLUoNMzpLUnvKCoCQ1yJ6zJDXItoYkNcjKWZIaZOUsSQ2ycpakBi379G1Jao+VsyQ1yJ6zJDXoYqmck/wco6fK7mD03KtTwJGqOrHNc5Okzeukcp74mKokvwc8wegBhc8Bzw+vP5vkge2fniRt0oweUzVvG1XO9wA3VdU/u1k9ySeB48BDax00PF58EeC/3//r3PMrH5jBVCVpChfJao1V4F8D/++c+MKwbU3jjxv/x6MHN3wEuCTNTPWRcjZKzvcDR5O8CrwxxP4N8DPAx7ZzYpK0JZ30nCcm56r6syQ/C9zK6IJggCXg+bGnykpSOy6G5AxQVavA134Ec5Gk89f4hb5puc5ZUl9W+vifepOzpL5cLG0NSbqgmJwlqUGd9Jwn3iEoSReaWq2pxyRJdiX5cpITSY4n+fgQ/8Mkf5PkhWF8aOyYB5OcTPJKkjvG4rckeXHY9nCSbPR7WDlL6svs2hrLwO9W1TeSvBv4epKnh21/UlX/dXznJDcC+4GbGN2897+T/Oyw7PhRRndNfw34ErAXeGrSyU3Okvoyo9UaVXUaOD28PpvkBKP7PdazD3iiqt4CXktyErg1yevAVVX1DECSx4E72SA529aQ1JfV1alHksUkx8bG4lofmeR64L3As0PoY0m+meTTSa4eYjt4505qGN2wt2MYS2vEJzI5S+rLJpJzVR2sqveNjYPnflySK4HPAfdX1fcYtSh+GtjDqLL+47d3XWM2NSE+kW0NSX2Z4RcfJbmMUWL+TFV9fvTx9ebY9j8Fvji8XQJ2jR2+k9H33y8Nr8+NT2TlLKkvm6icJxlWVHwKOFFVnxyLL4zt9qvAS8PrI8D+JJcnuQHYDTw39K7PJrlt+My7gCc3+jWsnCX1ZYMlcpvwfuCjwItJXhhivw98JMkeRq2J14HfAqiq40kOAy8zWulx39gXxN0LPAZcwehC4MSLgWByltSb2a3W+AvW7hd/acIxB4ADa8SPATdv5vwmZ0ldKW/flqQGza6tMVcmZ0l96eS7NUzOkvpi5SxJDVr2y/YlqT22NSSpQbY1JKk9LqWTpBZZOUtSg0zOktSgGd2+PW8mZ0ld2ejZgBcKk7OkvpicJalBrtaQpAZZOUtSg0zOktSeWrGtMZXr/vND232Ki94fLfzSvKfQvYe+++y8p6BpWTlLUntcSidJLTI5S1KD+mg5m5wl9aWW+8jOJmdJfekjN5ucJfXFC4KS1KJOKudL5j0BSZqlWq2pxyRJdiX5cpITSY4n+fgQf0+Sp5O8Ovy8euyYB5OcTPJKkjvG4rckeXHY9nCSbPR7mJwl9WV1E2OyZeB3q+rngduA+5LcCDwAHK2q3cDR4T3Dtv3ATcBe4JEklw6f9SiwCOwext6NTm5yltSVWp5+TPycqtNV9Y3h9VngBLAD2AccGnY7BNw5vN4HPFFVb1XVa8BJ4NYkC8BVVfVMVRXw+Ngx67LnLKkrtQ095yTXA+8FngWuq6rTMErgSa4ddtsBfG3ssKUh9oPh9bnxiaycJfVlE22NJItJjo2NxXM/LsmVwOeA+6vqexPOvFYfuSbEJ7JyltSVzVTOVXUQOLje9iSXMUrMn6mqzw/hN5MsDFXzAnBmiC8Bu8YO3wmcGuI714hPZOUsqSu1Ov2YZFhR8SngRFV9cmzTEeDu4fXdwJNj8f1JLk9yA6MLf88NLZCzSW4bPvOusWPWZeUsqSu1suEqtWm9H/go8GKSF4bY7wMPAYeT3AN8G/gwQFUdT3IYeJnRSo/7qurtR4HfCzwGXAE8NYyJTM6SujKrC4JV9Res3S8GuH2dYw4AB9aIHwNu3sz5Tc6SulKrM6uc58rkLKkr27GUbh5MzpK6UmXlLEnNsXKWpAatzm61xlyZnCV1xQuCktQgk7MkNaj6eBCKyVlSX6ycJalBLqWTpAatuFpDktpj5SxJDbLnLEkNcrWGJDXIylmSGrSy2scDnkzOkrpiW0OSGrTqag1Jak8vS+m23JxJ8huznIgkzULV9KNl59M5/8R6G5IsJjmW5Ng/LX/vPE4hSZuzWpl6tGxiWyPJN9fbBFy33nFVdRA4CHDVu36q8X+fJPXkYlmtcR1wB/D358QD/OW2zEiSzkMv1eBGyfmLwJVV9cK5G5J8ZVtmJEnnofV2xbQmJuequmfCtl+b/XQk6fz0slrDpXSSutLJw7fPa7WGJDWnyNRjI0k+neRMkpfGYn+Y5G+SvDCMD41tezDJySSvJLljLH5LkheHbQ8n2fDkJmdJXVmuTD2m8Biwd434n1TVnmF8CSDJjcB+4KbhmEeSXDrs/yiwCOwexlqf+c+YnCV1ZZaVc1V9FfjulKfeBzxRVW9V1WvASeDWJAvAVVX1TFUV8Dhw50YfZnKW1JXVTYzxG+aGsTjlaT6W5JtD2+PqIbYDeGNsn6UhtmN4fW58IpOzpK5spnKuqoNV9b6xcXCKUzwK/DSwBzgN/PEQX6sUrwnxiVytIakr271ao6refPt1kj9ldD8IjCriXWO77gRODfGda8QnsnKW1JUVMvXYiqGH/LZfBd5eyXEE2J/k8iQ3MLrw91xVnQbOJrltWKVxF/DkRuexcpbUlVk+pSrJZ4EPAtckWQL+APhgkj2MWhOvA78FUFXHkxwGXgaWgfuqamX4qHsZrfy4AnhqGBOZnCV1ZXWLFfFaquoja4Q/NWH/A8CBNeLHgJs3c26Ts6SuXCxffCRJF5Rebt82OUvqyurGd0ZfEEzOkrqysvEuFwSTs6SuzHK1xjyZnCV1ZZarNebJ5CypK67WkKQG2daQpAa5lE6SGrRi5SxJ7bFylqQGmZwlqUHTPRqwfSZnSV2xcpakBnn7tiQ1yHXOktQg2xqS1CCTsyQ1yO/WkKQG2XOWpAa5WmNK77rsx7f7FBe9//LmV+c9he6dXfrKvKegKa120tiwcpbUFS8ISlKD+qibTc6SOmPlLEkNWk4ftfMl856AJM1SbWJsJMmnk5xJ8tJY7D1Jnk7y6vDz6rFtDyY5meSVJHeMxW9J8uKw7eEkGy74MzlL6srqJsYUHgP2nhN7ADhaVbuBo8N7ktwI7AduGo55JMmlwzGPAovA7mGc+5k/xOQsqSur1NRjI1X1VeC754T3AYeG14eAO8fiT1TVW1X1GnASuDXJAnBVVT1TVQU8PnbMukzOkrqymbZGksUkx8bG4hSnuK6qTgMMP68d4juAN8b2WxpiO4bX58Yn8oKgpK5sZrVGVR0EDs7o1Gv1kWtCfCKTs6SurGz/Suc3kyxU1emhZXFmiC8Bu8b22wmcGuI714hPZFtDUldmfEFwLUeAu4fXdwNPjsX3J7k8yQ2MLvw9N7Q+zia5bVilcdfYMeuycpbUlZph5Zzks8AHgWuSLAF/ADwEHE5yD/Bt4MMAVXU8yWHgZWAZuK+q3v4epnsZrfy4AnhqGBOZnCV1ZZZ3CFbVR9bZdPs6+x8ADqwRPwbcvJlzm5wldcVvpZOkBvWRmk3Okjqz3El6NjlL6sosLwjOk8lZUlf8ylBJapCVsyQ1yMpZkhq0UlbOktQc1zlLUoPsOUtSg+w5S1KDbGtIUoNsa0hSg1ytIUkNsq0hSQ3ygqAkNciesyQ1yLaGJDWovCAoSe1ZsXKWpPb00ta4ZKMdkvxcktuTXHlOfO/2TUuStqaqph4tm5ick/wO8CTw28BLSfaNbf6j7ZyYJG3FKjX1aNlGbY3fBG6pqu8nuR74n0mur6r/BmS9g5IsAosAV13xr/iJH7t6RtOVpMkulqV0l1bV9wGq6vUkH2SUoP8tE5JzVR0EDgIs/OSNffylJF0Qerl9e6Oe83eS7Hn7zZCofwW4BviF7ZyYJG3FLNsaSV5P8mKSF5IcG2LvSfJ0kleHn1eP7f9gkpNJXklyx/n8Hhsl57uA74wHqmq5qu4CPnA+J5ak7bANPedfqqo9VfW+4f0DwNGq2g0cHd6T5EZgP3ATsBd4JMmlW/09Jibnqlqqqu+ss+3/bPWkkrRdfgSrNfYBh4bXh4A7x+JPVNVbVfUacBK4dasn2XApnSRdSDZTOSdZTHJsbCye83EF/K8kXx/bdl1VnQYYfl47xHcAb4wduzTEtsSbUCR1ZTOrNcYXL6zj/VV1Ksm1wNNJ/mrCvmstkthyeW5yltSVlZrdl4ZW1anh55kkX2DUpngzyUJVnU6yAJwZdl8Cdo0dvhM4tdVz29aQ1JVZ9ZyTvCvJu99+DfxH4CXgCHD3sNvdjG7UY4jvT3J5khuA3cBzW/09rJwldWWGd/5dB3whCYxy5f+oqj9L8jxwOMk9wLeBDwNU1fEkh4GXgWXgvqpa2erJTc6SujKrOwSr6lvAL64R/zvg9nWOOQAcmMX5Tc6SurLayR2CJmdJXblYvltDki4os1ytMU8mZ0ldsa0hSQ2yrSFJDbJylqQGWTlLUoNWtn7fR1NMzpK60vqDW6dlcpbUldYf3Dotk7Okrlg5S1KDXK0hSQ1ytYYkNcjbtyWpQfacJalB9pwlqUFWzpLUINc5S1KDrJwlqUGu1pCkBnlBUJIaZFtDkhrkHYKS1CArZ0lqUC895/Tyr8wsJVmsqoPznkfP/BtvP//GF7ZL5j2BRi3OewIXAf/G28+/8QXM5CxJDTI5S1KDTM5rs0+3/fwbbz//xhcwLwhKUoOsnCWpQSZnSWqQyXlMkr1JXklyMskD855Pj5J8OsmZJC/Ney69SrIryZeTnEhyPMnH5z0nbZ4950GSS4G/Bv4DsAQ8D3ykql6e68Q6k+QDwPeBx6vq5nnPp0dJFoCFqvpGkncDXwfu9L/lC4uV8ztuBU5W1beq6p+AJ4B9c55Td6rqq8B35z2PnlXV6ar6xvD6LHAC2DHfWWmzTM7v2AG8MfZ+Cf+D1gUuyfXAe4Fn5zsTbZbJ+R1ZI2bPRxesJFcCnwPur6rvzXs+2hyT8zuWgF1j73cCp+Y0F+m8JLmMUWL+TFV9ft7z0eaZnN/xPLA7yQ1JfgzYDxyZ85ykTUsS4FPAiar65Lzno60xOQ+qahn4GPDnjC6gHK6q4/OdVX+SfBZ4Bvh3SZaS3DPvOXXo/cBHgV9O8sIwPjTvSWlzXEonSQ2ycpakBpmcJalBJmdJapDJWZIaZHKWpAaZnCWpQSZnSWrQ/wdFNl4BBnFUTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.DataFrame(confusion_mat, index = list(encoded_classes),columns = list(encoded_classes))\n",
    "sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./Corona_NLP_test.csv', encoding='latin-1')\n",
    "\n",
    "test_df['hashtags'] = test_df['OriginalTweet'].apply(lambda x : extract_hash_tags(x))\n",
    "\n",
    "test_df['mentions'] = test_df['OriginalTweet'].apply(lambda x : extract_mentions(x))\n",
    "\n",
    "ytest = test_df['Sentiment']\n",
    "\n",
    "ytest.replace(encoding, inplace=True)\n",
    "\n",
    "test_df['encoded_sentiment'] = encoder.fit(ytest)\n",
    "\n",
    "test_df['OriginalTweet'] = test_df['OriginalTweet'].apply(lambda x: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split()))\n",
    "\n",
    "xtest = test_df.OriginalTweet\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    xtest, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=50, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(ytest.values)\n",
    "\n",
    "\n",
    "# Pytorch TensorDataset Instance\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_testing = DataLoader(dataset_test, \n",
    "                                   sampler=SequentialSampler(dataset_test), \n",
    "                                   batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss =  0.4181562294562658\n",
      "Test F1 =  0.8562025611432103\n"
     ]
    }
   ],
   "source": [
    "test_loss, predictions, true_vals = evaluate(dataloader_testing)\n",
    "test_f1 = f1_score_func(predictions, true_vals)\n",
    "print('Test Loss = ', test_loss)\n",
    "print('Test F1 = ', test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score =  0.8570300157977883\n"
     ]
    }
   ],
   "source": [
    "encoded_classes = encoder.classes_\n",
    "predicted_category = [encoded_classes[np.argmax(x)] for x in predictions]\n",
    "true_category = [encoded_classes[x] for x in true_vals]\n",
    "x = 0\n",
    "for i in range(len(true_category)):\n",
    "    if true_category[i] == predicted_category[i]:\n",
    "        x += 1\n",
    "        \n",
    "print('Accuracy Score = ', x / len(true_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./predictions/bert.csv\", predictions, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
